{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hadare/Documents/CodingProjects/SBERT/sbert\n",
      "3.10.14 (main, Mar 19 2024, 21:46:16) [Clang 15.0.0 (clang-1500.3.9.4)]\n",
      "/Users/hadare/Documents/CodingProjects/SBERT/sbert/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "print(os.environ.get('VIRTUAL_ENV'))\n",
    "print(sys.version)\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using MPS\n"
     ]
    }
   ],
   "source": [
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    print(\"using MPS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_labels</th>\n",
       "      <th>genre</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>pairID</th>\n",
       "      <th>promptID</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>translation1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>translation2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['neutral']</td>\n",
       "      <td>fiction</td>\n",
       "      <td>neutral</td>\n",
       "      <td>128144n</td>\n",
       "      <td>128144</td>\n",
       "      <td>She smiled back.</td>\n",
       "      <td>היא חייכה בחזרה.</td>\n",
       "      <td>She was so happy she couldn't stop smiling.</td>\n",
       "      <td>היא היתה כל כך שמחה שהיא לא יכלה להפסיק לחייך.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['neutral']</td>\n",
       "      <td>slate</td>\n",
       "      <td>neutral</td>\n",
       "      <td>145721n</td>\n",
       "      <td>145721</td>\n",
       "      <td>But such a show would have meant the museum ta...</td>\n",
       "      <td>אבל מופע כזה היה אומר שהמוזיאון יסתכל טוב על ה...</td>\n",
       "      <td>The museum didn't want to look too closely int...</td>\n",
       "      <td>המוזיאון לא רצה להתבונן מקרוב מדי בחלקו בעולם ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['entailment']</td>\n",
       "      <td>travel</td>\n",
       "      <td>entailment</td>\n",
       "      <td>79085e</td>\n",
       "      <td>79085</td>\n",
       "      <td>In June you can watch dealers haggling over he...</td>\n",
       "      <td>בחודש יוני אפשר לצפות בסוחרים מתמקחים על ערמות...</td>\n",
       "      <td>During the summer months dealers barter for pi...</td>\n",
       "      <td>בחודשי הקיץ סוחרים סוחרים בערימות של פקעות תול...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['contradiction']</td>\n",
       "      <td>fiction</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>133206c</td>\n",
       "      <td>133206</td>\n",
       "      <td>119 Cynthia was back from the hospital, and I ...</td>\n",
       "      <td>סינתיה חזרה מבית החולים, ואני הצבתי את הכיסא ש...</td>\n",
       "      <td>Poirot talked to Cynthia himself, asking to vi...</td>\n",
       "      <td>פוארו דיבר עם סינתיה בעצמו, וביקש לבקר אותה בע...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['contradiction']</td>\n",
       "      <td>slate</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>107785c</td>\n",
       "      <td>107785</td>\n",
       "      <td>Oh well, if we're not destined for matrimony i...</td>\n",
       "      <td>ובכן, אם לא נועדנו לנישואין בחיים האלה, אולי ה...</td>\n",
       "      <td>If we are married, I will buy you a beer or a ...</td>\n",
       "      <td>אם נתחתן, אקנה לך בירה או ד\"ר פפר.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    annotator_labels    genre     gold_label   pairID  promptID  \\\n",
       "0        ['neutral']  fiction        neutral  128144n    128144   \n",
       "1        ['neutral']    slate        neutral  145721n    145721   \n",
       "2     ['entailment']   travel     entailment   79085e     79085   \n",
       "3  ['contradiction']  fiction  contradiction  133206c    133206   \n",
       "4  ['contradiction']    slate  contradiction  107785c    107785   \n",
       "\n",
       "                                           sentence1  \\\n",
       "0                                   She smiled back.   \n",
       "1  But such a show would have meant the museum ta...   \n",
       "2  In June you can watch dealers haggling over he...   \n",
       "3  119 Cynthia was back from the hospital, and I ...   \n",
       "4  Oh well, if we're not destined for matrimony i...   \n",
       "\n",
       "                                        translation1  \\\n",
       "0                                  היא חייכה בחזרה.    \n",
       "1  אבל מופע כזה היה אומר שהמוזיאון יסתכל טוב על ה...   \n",
       "2  בחודש יוני אפשר לצפות בסוחרים מתמקחים על ערמות...   \n",
       "3  סינתיה חזרה מבית החולים, ואני הצבתי את הכיסא ש...   \n",
       "4  ובכן, אם לא נועדנו לנישואין בחיים האלה, אולי ה...   \n",
       "\n",
       "                                           sentence2  \\\n",
       "0        She was so happy she couldn't stop smiling.   \n",
       "1  The museum didn't want to look too closely int...   \n",
       "2  During the summer months dealers barter for pi...   \n",
       "3  Poirot talked to Cynthia himself, asking to vi...   \n",
       "4  If we are married, I will buy you a beer or a ...   \n",
       "\n",
       "                                        translation2  \n",
       "0     היא היתה כל כך שמחה שהיא לא יכלה להפסיק לחייך.  \n",
       "1  המוזיאון לא רצה להתבונן מקרוב מדי בחלקו בעולם ...  \n",
       "2  בחודשי הקיץ סוחרים סוחרים בערימות של פקעות תול...  \n",
       "3  פוארו דיבר עם סינתיה בעצמו, וביקש לבקר אותה בע...  \n",
       "4                 אם נתחתן, אקנה לך בירה או ד\"ר פפר.  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = load_dataset('csv', './data/train.csv')\n",
    "df_train = pd.read_csv('data/test.csv')\n",
    "df_valid = pd.read_csv('data/dev.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "df_train.head()\n",
    "df_valid.head()\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_valid = Dataset.from_pandas(df_valid)\n",
    "dataset_test = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to process the dataset\n",
    "def preprocess_data(data):\n",
    "    premise = data['translation1']\n",
    "    hypothesis = data['translation2']\n",
    "    label = data['gold_label']\n",
    "    \n",
    "    # Convert labels to numerical values if needed\n",
    "    label_mapping = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
    "    label = label_mapping.get(label, -1)  # -1 for any unknown labels\n",
    "\n",
    "    return premise, hypothesis, label\n",
    "\n",
    "# Apply preprocessing to the train, validation, and test datasets\n",
    "train_data = [preprocess_data(row) for row in dataset_train]\n",
    "valid_data = [preprocess_data(row) for row in dataset_valid]\n",
    "test_data = [preprocess_data(row) for row in dataset_test]\n",
    "\n",
    "# Convert to pandas DataFrame for easier manipulation\n",
    "train_df = pd.DataFrame(train_data, columns=['premise', 'hypothesis', 'label'])\n",
    "valid_df = pd.DataFrame(valid_data, columns=['premise', 'hypothesis', 'label'])\n",
    "test_df = pd.DataFrame(test_data, columns=['premise', 'hypothesis', 'label'])\n",
    "\n",
    "# Save processed data if needed\n",
    "# train_df.to_csv('train.csv', index=False)\n",
    "# valid_df.to_csv('valid.csv', index=False)\n",
    "# test_df.to_csv('test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df_train.isnull().sum().sum())\n",
    "print(df_valid.isnull().sum().sum())\n",
    "print(df_test.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_to_lower = ['sentence1', 'sentence2']\n",
    "# df_train[columns_to_lower] = df_train[columns_to_lower].map(lambda x: x.lower())\n",
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, BertTokenizer\n",
    "\n",
    "# tokenizer_aleph = AutoTokenizer.from_pretrained('onlplab/alephbert-base')\n",
    "# tokenizer_mbert = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer_aleph)\n",
    "# tokenizer_mbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name dicta-il/dictabert. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dadbd497ae4448d589d1e9babd50f418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ee6326f4f544c0835379b878be5cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/738M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dicta-il/dictabert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9a2b43003b4d7f83d134e5db77a1f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce00bdf9a0ad4c31a0a049e556737d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/1.50M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721504acf99e47ec8bdcfeaa81d0c081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.58M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentencesDataset, losses\n",
    "from sentence_transformers.readers import InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the base model\n",
    "model_name = 'onlplab/alephbert-base'  # e.g., 'onlplab/alephbert-base' \n",
    "# model_name = 'google-bert/bert-base-multilingual-cased'\n",
    "# model_name = 'dicta-il/dictabert'\n",
    "model = SentenceTransformer(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare InputExamples\n",
    "train_examples = [InputExample(texts=[row['premise'], row['hypothesis']], label=row['label']) for _, row in train_df.iterrows()]\n",
    "valid_examples = [InputExample(texts=[row['premise'], row['hypothesis']], label=row['label']) for _, row in valid_df.iterrows()]\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = SentencesDataset(train_examples, model)\n",
    "valid_dataset = SentencesDataset(valid_examples, model)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=32)\n",
    "valid_dataloader = DataLoader(valid_dataset, shuffle=False, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45c81f7c90d4c3596a8f29b39f7510e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2932464f144f5eaa436691250037af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2226, 'grad_norm': 1.0029336214065552, 'learning_rate': 4.732824427480917e-06, 'epoch': 3.18}\n",
      "{'train_runtime': 470.2055, 'train_samples_per_second': 42.535, 'train_steps_per_second': 1.336, 'train_loss': 0.17946006139372564, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "# Define loss function\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 4\n",
    "warmup_steps = 100  # Adjust based on dataset size\n",
    "\n",
    "# valid_examples = []\n",
    "\n",
    "# # Extract examples from the DataLoader\n",
    "# for batch in valid_dataloader:\n",
    "#     sentences1, sentences2, scores = batch\n",
    "#     for sent1, sent2, score in zip(sentences1, sentences2, scores):\n",
    "#         sent1 = sent1 if isinstance(sent1, str) else sent1.item()\n",
    "#         sent2 = sent2 if isinstance(sent2, str) else sent2.item()\n",
    "#         score = score.item()\n",
    "#         valid_examples.append(InputExample(texts=[sent1, sent2], label=score))\n",
    "\n",
    "# # Create the evaluator\n",
    "# evaluator = EmbeddingSimilarityEvaluator.from_input_examples(valid_examples, name='dev')\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    # evaluator=evaluator,\n",
    "    evaluator = EmbeddingSimilarityEvaluator.from_input_examples(valid_examples, name='dev'),\n",
    "    epochs=num_epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path='output/sbert_hebrew_model'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained('./trained_sbert_model')\n",
    "# tokenizer.save_pretrained('./trained_sbert_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sid', 'score', 'sentence1', 'sentence2'],\n",
      "        num_rows: 1379\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(sts_data)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Prepare the evaluation dataset\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m sts_examples \u001b[38;5;241m=\u001b[39m [InputExample(texts\u001b[38;5;241m=\u001b[39m[row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence1\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence2\u001b[39m\u001b[38;5;124m'\u001b[39m]], label\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m sts_data]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m     10\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m EmbeddingSimilarityEvaluator\u001b[38;5;241m.\u001b[39mfrom_input_examples(sts_examples, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhebrew-sts\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(sts_data)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Prepare the evaluation dataset\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m sts_examples \u001b[38;5;241m=\u001b[39m [InputExample(texts\u001b[38;5;241m=\u001b[39m[\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence2\u001b[39m\u001b[38;5;124m'\u001b[39m]], label\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m sts_data]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m     10\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m EmbeddingSimilarityEvaluator\u001b[38;5;241m.\u001b[39mfrom_input_examples(sts_examples, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhebrew-sts\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "# Load the Hebrew STS benchmark dataset (replace with actual data loading)\n",
    "# sts_data = load_dataset('./data/heb_sts_test.csv')\n",
    "sts_data = load_dataset('csv', data_files='./data/heb_sts_test.csv')\n",
    "print(sts_data)\n",
    "\n",
    "# Prepare the evaluation dataset\n",
    "sts_examples = [InputExample(texts=[row['sentence1'], row['sentence2']], label=row['score']) for row in sts_data]\n",
    "\n",
    "# Evaluate\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(sts_examples, name='hebrew-sts')\n",
    "model.evaluate(evaluator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for the model on Hebrew STS benchmark:\n",
      "{'hebrew-sts_pearson_cosine': 0.7116431915772918, 'hebrew-sts_spearman_cosine': 0.6906499377642947, 'hebrew-sts_pearson_manhattan': 0.7160632824099897, 'hebrew-sts_spearman_manhattan': 0.6995660820383547, 'hebrew-sts_pearson_euclidean': 0.7167087784665823, 'hebrew-sts_spearman_euclidean': 0.7002192924941905, 'hebrew-sts_pearson_dot': 0.6435424677387844, 'hebrew-sts_spearman_dot': 0.6197261353452318, 'hebrew-sts_pearson_max': 0.7167087784665823, 'hebrew-sts_spearman_max': 0.7002192924941905}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import InputExample\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "sts_data = load_dataset('csv', data_files='./data/heb_sts_test.csv')\n",
    "\n",
    "# Access the dataset\n",
    "dataset = sts_data['train']  # or the appropriate split name\n",
    "\n",
    "# Convert to DataFrame for inspection\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Prepare the evaluation dataset\n",
    "sts_examples = [InputExample(texts=[row['sentence1'], row['sentence2']], label=row['score']) \n",
    "                for row in df.to_dict(orient='records')]\n",
    "\n",
    "# Initialize the evaluator\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(sts_examples, name='hebrew-sts')\n",
    "\n",
    "# Assuming you have a trained model\n",
    "model.evaluate(evaluator)\n",
    "\n",
    "\n",
    "print(\"Evaluation results for the model on Hebrew STS benchmark:\")\n",
    "print(evaluator(model, output_path='output/sbert_hebrew_model'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
