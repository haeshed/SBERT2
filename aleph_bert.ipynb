{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries if not already installed\n",
    "# !pip install transformers datasets pandas\n",
    "# !pip install matplotlib\n",
    "\n",
    "# Import necessary libraries\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from sentence_transformers import SentenceTransformer, losses\n",
    "import pandas as pd\n",
    "import torch\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 14:48:17 - Using MPS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO)\n",
    "\n",
    "# Check for MPS and CUDA availability\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"MPS is available\")\n",
    "    logging.info(\"Using MPS\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"CUDA is available\")\n",
    "    logging.info(\"Using CUDA\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "    logging.info(\"Using CPU\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 14:48:17 - Loading datasets from CSV files\n",
      "2024-10-18 14:48:18 - Datasets loaded successfully\n",
      "2024-10-18 14:48:18 - Converting DataFrames to Hugging Face Dataset objects\n",
      "2024-10-18 14:48:19 - Conversion completed successfully\n",
      "2024-10-18 14:48:19 - Train dataset size: 293298\n",
      "2024-10-18 14:48:19 - Validation dataset size: 5000\n",
      "2024-10-18 14:48:19 - Test dataset size: 5000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    # Load the datasets from CSV files\n",
    "    logging.info(\"Loading datasets from CSV files\")\n",
    "    train_df = pd.read_csv('data/train.csv')\n",
    "    valid_df = pd.read_csv('data/dev.csv')\n",
    "    test_df = pd.read_csv('data/test.csv')\n",
    "    logging.info(\"Datasets loaded successfully\")\n",
    "\n",
    "    # Convert pandas DataFrames to Hugging Face Dataset objects\n",
    "    logging.info(\"Converting DataFrames to Hugging Face Dataset objects\")\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    valid_dataset = Dataset.from_pandas(valid_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "    logging.info(\"Conversion completed successfully\")\n",
    "\n",
    "    # Log some information about the datasets\n",
    "    logging.info(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    logging.info(f\"Validation dataset size: {len(valid_dataset)}\")\n",
    "    logging.info(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {str(e)}\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer for BERT\n",
    "model_name = 'onlplab/alephbert-base'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "label_mapping = {'contradiction': -1, 'entailment': 1, 'neutral': 0}\n",
    "\n",
    "\n",
    "# Define a function to preprocess the input data\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the inputs (translation1 and translation2)\n",
    "    inputs = tokenizer(\n",
    "        examples['translation1'], \n",
    "        examples['translation2'], \n",
    "        truncation=False, \n",
    "        padding='max_length', \n",
    "        max_length=128  # Or another max length depending on your needs\n",
    "    )\n",
    "    \n",
    "    # Map the annotator_labels to numerical labels if needed\n",
    "    # inputs['labels'] = examples['gold_label']\n",
    "    inputs['labels'] = [label_mapping[label] for label in examples['gold_label']]\n",
    "\n",
    "    return inputs\n",
    "\n",
    "# Apply the preprocessing function to the datasets\n",
    "train_dataset = train_dataset.map(preprocess_function,batched=True)\n",
    "# train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "valid_dataset = valid_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Load the pretrained BERT model for sequence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)  # Change num_labels to match the number of classes in your NLI task\n",
    "# tokenizer = BertTokenizerFast.from_pretrained('onlplab/alephbert-base')\n",
    "# model = BertModel.from_pretrained('onlplab/alephbert-base')\n",
    "# Create a data collator that will dynamically pad the inputs during training\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Set up the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",          # Output directory\n",
    "    evaluation_strategy=\"epoch\",     # Evaluation is done at the end of each epoch\n",
    "    learning_rate=2e-5,              # Learning rate\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    num_train_epochs=3,              # Number of epochs to train\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # The instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # Training arguments\n",
    "    train_dataset=train_dataset,         # Training dataset\n",
    "    eval_dataset=valid_dataset,          # Evaluation dataset\n",
    "    tokenizer=tokenizer,                 # Tokenizer\n",
    "    data_collator=data_collator,          # Data collator\n",
    "    compute_metrics=compute_metrics      # Custom metrics function\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"Evaluation Results: {eval_results}\")\n",
    "\n",
    "# Fine-Tune the Model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"Evaluation Results: {eval_results}\")\n",
    "\n",
    "# Save the fine-tuned model for future use\n",
    "output_dir = \"output/_\" + model_name.replace(\"/\", \"-\") + \"-\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "# Print the test results\n",
    "print(f\"Test Results: {test_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Extract the predicted labels (argmax of logits)\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Extract the true labels\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "# Optional: Plot the confusion matrix using ConfusionMatrixDisplay from sklearn\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=list(label_mapping.keys()))\n",
    "disp.plot(cmap='Blues')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.trace(conf_matrix) / np.sum(conf_matrix)\n",
    "\n",
    "print(f\"Accuracy from Confusion Matrix: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming predictions and true labels are already computed\n",
    "predictions = trainer.predict(test_dataset)\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Binarize the true labels for ROC-AUC computation (necessary for multi-class)\n",
    "n_classes = len(label_mapping)  # Number of classes (e.g., 3 for 'contradiction', 'entailment', 'neutral')\n",
    "true_labels_binarized = label_binarize(true_labels, classes=[0, 1, 2])\n",
    "\n",
    "# Calculate Precision, Recall, and F1-Score for each class and overall (micro and macro)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average=None, labels=[0, 1, 2])\n",
    "precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='micro')\n",
    "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "# Print Precision, Recall, and F1-Score for each class\n",
    "print(f\"Per-class Precision, Recall, F1-Score:\")\n",
    "for idx, label in enumerate(label_mapping.keys()):\n",
    "    print(f\"Class {label} - Precision: {precision[idx]:.4f}, Recall: {recall[idx]:.4f}, F1-Score: {f1[idx]:.4f}\")\n",
    "\n",
    "# Print overall metrics\n",
    "print(f\"\\nOverall Micro-Averaged Metrics - Precision: {precision_micro:.4f}, Recall: {recall_micro:.4f}, F1-Score: {f1_micro:.4f}\")\n",
    "print(f\"Overall Macro-Averaged Metrics - Precision: {precision_macro:.4f}, Recall: {recall_macro:.4f}, F1-Score: {f1_macro:.4f}\")\n",
    "\n",
    "# Compute ROC-AUC for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(true_labels_binarized[:, i], predictions.predictions[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "# Aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Interpolate all ROC curves at these points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot ROC curve for each class and the macro-average\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"], label=f'Macro-average ROC curve (area = {roc_auc[\"macro\"]:.2f})', color='navy', linestyle=':', linewidth=2)\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], lw=2, label=f'ROC curve of class {list(label_mapping.keys())[i]} (area = {roc_auc[i]:.2f})')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curves for Multi-Class NLI Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
