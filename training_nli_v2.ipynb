{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.similarity_functions import SimilarityFunction\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import BatchSamplers, SentenceTransformerTrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the log level to INFO to get more information\n",
    "logging.basicConfig(format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO)\n",
    "\n",
    "model_name = \"imvladikon/sentence-transformers-alephbert\"\n",
    "train_batch_size = 128  # The larger you select this, the better the results (usually). But it requires more GPU memory\n",
    "max_seq_length = 75\n",
    "num_epochs = 1\n",
    "\n",
    "# Save path of the model\n",
    "output_dir = \"output/training_nli_v2_\" + model_name.replace(\"/\", \"-\") + \"-\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 12:50:08 - Use pytorch device_name: mps\n",
      "2024-10-18 12:50:08 - Load pretrained SentenceTransformer: imvladikon/sentence-transformers-alephbert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38aeb6188244a6aa4ad23a82658306f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02c4a8ce3654349b1864367a565b1a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3f621eef244c998b3766ea48b2ab62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35eb2099abd740bc8e29ad548e831e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85d6afa306c4d349432b667b227e1ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca44b10179a74284ab6a51ea860fcbe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/504M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5ca5d59e594fcc8e0cfcf3507523aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/379 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae51bfa7fb24c2596430452e7d6442f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/545k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96db3bbfb89849a89eb4a83c1378f810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdff186ec1a94368b3491e00756a27d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadare/Documents/CodingProjects/SBERT2/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c550be5abbc34fc0b799ed8fa89b3bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 1. Here we define our SentenceTransformer model. If not already a Sentence Transformer model, it will automatically\n",
    "# create one with \"mean\" pooling.\n",
    "model = SentenceTransformer(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 12:57:37 - Load the datasets from CSV files\n",
      "2024-10-18 12:57:38 - Convert pandas DataFrames to Hugging Face Dataset objects\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Load the AllNLI dataset: https://huggingface.co/datasets/sentence-transformers/all-nli\n",
    "# We'll start with 10k training samples, but you can increase this to get a stronger model\n",
    "# logging.info(\"Read AllNLI train dataset\")\n",
    "# train_dataset = load_dataset(\"sentence-transformers/all-nli\", \"triplet\", split=\"train\").select(range(10000))\n",
    "# eval_dataset = load_dataset(\"sentence-transformers/all-nli\", \"triplet\", split=\"dev\").select(range(1000))\n",
    "# logging.info(train_dataset)\n",
    "\n",
    "logging.info(\"Load the datasets from CSV files\")\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "valid_df = pd.read_csv('data/dev.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "logging.info(\"Convert pandas DataFrames to Hugging Face Dataset objects\")\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "valid_dataset = Dataset.from_pandas(valid_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Define our training loss: https://sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 13:08:19 - Evaluation before training:\n",
      "2024-10-18 13:08:19 - EmbeddingSimilarityEvaluator: Evaluating the model on the sts dataset:\n",
      "2024-10-18 13:08:31 - Cosine-Similarity :\tPearson: 0.6693\tSpearman: 0.6508\n",
      "2024-10-18 13:08:31 - Manhattan-Distance:\tPearson: 0.6429\tSpearman: 0.6310\n",
      "2024-10-18 13:08:31 - Euclidean-Distance:\tPearson: 0.6439\tSpearman: 0.6317\n",
      "2024-10-18 13:08:31 - Dot-Product-Similarity:\tPearson: 0.4571\tSpearman: 0.4405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sts_pearson_cosine': np.float64(0.6693029706135705),\n",
       " 'sts_spearman_cosine': np.float64(0.6508239891280294),\n",
       " 'sts_pearson_manhattan': np.float64(0.6429249591660918),\n",
       " 'sts_spearman_manhattan': np.float64(0.6309807328394371),\n",
       " 'sts_pearson_euclidean': np.float64(0.6438809257962056),\n",
       " 'sts_spearman_euclidean': np.float64(0.6317416791343402),\n",
       " 'sts_pearson_dot': np.float64(0.45707904633475227),\n",
       " 'sts_spearman_dot': np.float64(0.44048541100202476),\n",
       " 'sts_pearson_max': np.float64(0.6693029706135705),\n",
       " 'sts_spearman_max': np.float64(0.6508239891280294)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 4. Define an evaluator for use during training. This is useful to keep track of alongside the evaluation loss.\n",
    "# stsb_eval_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"validation\")\n",
    "# dev_evaluator = EmbeddingSimilarityEvaluator(\n",
    "#     sentences1=stsb_eval_dataset[\"sentence1\"],\n",
    "#     sentences2=stsb_eval_dataset[\"sentence2\"],\n",
    "#     scores=stsb_eval_dataset[\"score\"],\n",
    "#     main_similarity=SimilarityFunction.COSINE,\n",
    "#     name=\"sts-dev\",\n",
    "# )\n",
    "# logging.info(\"Evaluation before training:\")\n",
    "# dev_evaluator(model)\n",
    "\n",
    "sts_eval_df = pd.read_csv('data/heb_sts_test.csv')\n",
    "stsb_eval_dataset = Dataset.from_pandas(sts_eval_df)\n",
    "\n",
    "dev_evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=stsb_eval_dataset[\"sentence1\"],\n",
    "    sentences2=stsb_eval_dataset[\"sentence2\"],\n",
    "    scores=stsb_eval_dataset[\"score\"],\n",
    "    main_similarity=SimilarityFunction.COSINE,\n",
    "    name=\"sts\",\n",
    ")\n",
    "logging.info(\"Evaluation before training:\")\n",
    "dev_evaluator(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Define the training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=output_dir,\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=train_batch_size,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=False,  # Set to True if you have a GPU that supports BF16\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    run_name=\"nli-v2\",  # Will be used in W&B if `wandb` is installed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Create the trainer & start training\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss=train_loss,\n",
    "    evaluator=dev_evaluator,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. Evaluate the model performance on the STS Benchmark test dataset\n",
    "test_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"test\")\n",
    "test_evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=test_dataset[\"sentence1\"],\n",
    "    sentences2=test_dataset[\"sentence2\"],\n",
    "    scores=test_dataset[\"score\"],\n",
    "    main_similarity=SimilarityFunction.COSINE,\n",
    "    name=\"sts-test\",\n",
    ")\n",
    "test_evaluator(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8. Save the trained & evaluated model locally\n",
    "final_output_dir = f\"{output_dir}/final\"\n",
    "model.save(final_output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
