{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./sbert/lib/python3.10/site-packages (4.43.3)\n",
      "Requirement already satisfied: filelock in ./sbert/lib/python3.10/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./sbert/lib/python3.10/site-packages (from transformers) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./sbert/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./sbert/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./sbert/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./sbert/lib/python3.10/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in ./sbert/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./sbert/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./sbert/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./sbert/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./sbert/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./sbert/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./sbert/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./sbert/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./sbert/lib/python3.10/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./sbert/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: torch in ./sbert/lib/python3.10/site-packages (2.4.0)\n",
      "Requirement already satisfied: filelock in ./sbert/lib/python3.10/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./sbert/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./sbert/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in ./sbert/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./sbert/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./sbert/lib/python3.10/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./sbert/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./sbert/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns: Index(['annotator_labels', 'genre', 'gold_label', 'pairID', 'promptID',\n",
      "       'sentence1', 'translation1', 'sentence2', 'translation2'],\n",
      "      dtype='object')\n",
      "Validation columns: Index(['annotator_labels', 'genre', 'gold_label', 'pairID', 'promptID',\n",
      "       'sentence1', 'translation1', 'sentence2', 'translation2'],\n",
      "      dtype='object')\n",
      "Test columns: Index(['annotator_labels', 'genre', 'gold_label', 'pairID', 'promptID',\n",
      "       'sentence1', 'translation1', 'sentence2', 'translation2'],\n",
      "      dtype='object')\n",
      "Train data preview:\n",
      "    annotator_labels    genre     gold_label   pairID  promptID  \\\n",
      "0     ['entailment']  fiction     entailment  134793e    134793   \n",
      "1     ['entailment']  fiction     entailment   37397e     37397   \n",
      "2     ['entailment']    slate     entailment    1069e      1069   \n",
      "3  ['contradiction']    slate  contradiction   23505c     23505   \n",
      "4  ['contradiction']   travel  contradiction   60529c     60529   \n",
      "\n",
      "                                           sentence1  \\\n",
      "0  One of our number will carry out your instruct...   \n",
      "1  How do you know? All this is their information...   \n",
      "2   (Read  for Slate 's take on Jackson's findings.)   \n",
      "3                                 Gays and lesbians.   \n",
      "4  At the end of Rue des Francs-Bourgeois is what...   \n",
      "\n",
      "                                        translation1  \\\n",
      "0           אחד מאיתנו יבצע את ההוראות שלך בדקדקנות.   \n",
      "1              מאיפה אתה יודע? כל זה שוב המידע שלהם.   \n",
      "2      (קראו את הדעה של Slate על הממצאים של ג'קסון.)   \n",
      "3                                   הומואים ולסביות.   \n",
      "4  בסוף רחוב דה פרנק-בורגואה נמצא מה שרבים מחשיבי...   \n",
      "\n",
      "                                           sentence2  \\\n",
      "0  A member of my team will execute your orders w...   \n",
      "1                  This information belongs to them.   \n",
      "2        Slate had an opinion on Jackson's findings.   \n",
      "3                                     Heterosexuals.   \n",
      "4  Place des Vosges is constructed entirely of gr...   \n",
      "\n",
      "                                  translation2  \n",
      "0  חבר בצוות שלי יבצע את הפקודות שלך בדיוק רב.  \n",
      "1                          המידע הזה שייך להם.  \n",
      "2       ל-Slate הייתה דעה על ממצאיו של ג'קסון.  \n",
      "3                                הטרוסקסואלים.  \n",
      "4            כיכר דה ווז בנויה כולה משיש אפור.  \n",
      "Validation data preview:\n",
      "    annotator_labels       genre     gold_label   pairID  promptID  \\\n",
      "0        ['neutral']      travel        neutral   52772n     52772   \n",
      "1        ['neutral']  government        neutral  141334n    141334   \n",
      "2  ['contradiction']       slate  contradiction   13806c     13806   \n",
      "3     ['entailment']       slate     entailment  131294e    131294   \n",
      "4     ['entailment']     fiction     entailment   19833e     19833   \n",
      "\n",
      "                                           sentence1  \\\n",
      "0  Thebes held onto power until the 12th Dynasty,...   \n",
      "1  Without the support of federal finance organiz...   \n",
      "2  The win brought tears to her eyes, of course, ...   \n",
      "3  The New York Times ' Janet Maslin says the fil...   \n",
      "4              The next witness was Mary Cavendish.    \n",
      "\n",
      "                                        translation1  \\\n",
      "0  תבאי החזיקה בכוח עד השושלת ה-12, כאשר מלכה הרא...   \n",
      "1  ללא תמיכת ארגוני המימון הפדרליים, מנהלי התוכני...   \n",
      "2  הניצחון הביא דמעות לעיניה, כמובן, ועורר שיחת ט...   \n",
      "3  ג'נט מסלין מהניו יורק טיימס אומרת שהסרט עובד ב...   \n",
      "4                       העדה הבאה הייתה מרי קוונדיש.   \n",
      "\n",
      "                                           sentence2  \\\n",
      "0  The capital near Memphis lasted only half a ce...   \n",
      "1  Federal finance organizations are the best sou...   \n",
      "2  She called her father after she lost, smiling ...   \n",
      "3       The movie stars Matt Dillon and Gary Sinise.   \n",
      "4                         There was another witness.   \n",
      "\n",
      "                                        translation2  \n",
      "0  הבירה ליד ממפיס החזיקה מעמד רק חצי מאה לפני שת...  \n",
      "1  ארגוני מימון פדרליים הם המקור הטוב ביותר למידע...  \n",
      "2  היא התקשרה לאביה לאחר שהפסידה, מחייכת בזמן שעש...  \n",
      "3               כוכבי הקולנוע מאט דילון וגארי סיניז.  \n",
      "4                                       היה עד נוסף.  \n",
      "Test data preview:\n",
      "    annotator_labels    genre     gold_label   pairID  promptID  \\\n",
      "0        ['neutral']  fiction        neutral  128144n    128144   \n",
      "1        ['neutral']    slate        neutral  145721n    145721   \n",
      "2     ['entailment']   travel     entailment   79085e     79085   \n",
      "3  ['contradiction']  fiction  contradiction  133206c    133206   \n",
      "4  ['contradiction']    slate  contradiction  107785c    107785   \n",
      "\n",
      "                                           sentence1  \\\n",
      "0                                   She smiled back.   \n",
      "1  But such a show would have meant the museum ta...   \n",
      "2  In June you can watch dealers haggling over he...   \n",
      "3  119 Cynthia was back from the hospital, and I ...   \n",
      "4  Oh well, if we're not destined for matrimony i...   \n",
      "\n",
      "                                        translation1  \\\n",
      "0                                  היא חייכה בחזרה.    \n",
      "1  אבל מופע כזה היה אומר שהמוזיאון יסתכל טוב על ה...   \n",
      "2  בחודש יוני אפשר לצפות בסוחרים מתמקחים על ערמות...   \n",
      "3  סינתיה חזרה מבית החולים, ואני הצבתי את הכיסא ש...   \n",
      "4  ובכן, אם לא נועדנו לנישואין בחיים האלה, אולי ה...   \n",
      "\n",
      "                                           sentence2  \\\n",
      "0        She was so happy she couldn't stop smiling.   \n",
      "1  The museum didn't want to look too closely int...   \n",
      "2  During the summer months dealers barter for pi...   \n",
      "3  Poirot talked to Cynthia himself, asking to vi...   \n",
      "4  If we are married, I will buy you a beer or a ...   \n",
      "\n",
      "                                        translation2  \n",
      "0     היא היתה כל כך שמחה שהיא לא יכלה להפסיק לחייך.  \n",
      "1  המוזיאון לא רצה להתבונן מקרוב מדי בחלקו בעולם ...  \n",
      "2  בחודשי הקיץ סוחרים סוחרים בערימות של פקעות תול...  \n",
      "3  פוארו דיבר עם סינתיה בעצמו, וביקש לבקר אותה בע...  \n",
      "4                 אם נתחתן, אקנה לך בירה או ד\"ר פפר.  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the paths to the local files\n",
    "train_file = \"./data/train.csv\"\n",
    "validation_file = \"./data/dev.csv\"\n",
    "test_file = \"./data/test.csv\"\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv(train_file)\n",
    "validation_df = pd.read_csv(validation_file)\n",
    "test_df = pd.read_csv(test_file)\n",
    "\n",
    "# Display the columns in each DataFrame\n",
    "print(\"Train columns:\", train_df.columns)\n",
    "print(\"Validation columns:\", validation_df.columns)\n",
    "print(\"Test columns:\", test_df.columns)\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "print(\"Train data preview:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"Validation data preview:\")\n",
    "print(validation_df.head())\n",
    "\n",
    "print(\"Test data preview:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a function to preprocess the data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from transformers import AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# # Load the tokenizer\n",
    "# model_name = \"bert-base-multilingual-cased\"  # Change this to AlephBERT or DictaBERT as needed\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# # Define the paths to the local files\n",
    "# train_file = \"/Users/user/Library/Mobile Documents/com~apple~CloudDocs/2 24/NLP/final proj/data/train.csv\"\n",
    "# validation_file = \"/Users/user/Library/Mobile Documents/com~apple~CloudDocs/2 24/NLP/final proj/data/dev.csv\"\n",
    "# test_file = \"/Users/user/Library/Mobile Documents/com~apple~CloudDocs/2 24/NLP/final proj/data/test.csv\"\n",
    "\n",
    "# # Load the datasets\n",
    "# train_df = pd.read_csv(train_file)\n",
    "# validation_df = pd.read_csv(validation_file)\n",
    "# test_df = pd.read_csv(test_file)\n",
    "\n",
    "# # Display the columns in each DataFrame\n",
    "# print(\"Train columns:\", train_df.columns)\n",
    "# print(\"Validation columns:\", validation_df.columns)\n",
    "# print(\"Test columns:\", test_df.columns)\n",
    "\n",
    "# # Display the first few rows to understand the structure\n",
    "# print(\"Train data preview:\")\n",
    "# print(train_df.head())\n",
    "\n",
    "# print(\"Validation data preview:\")\n",
    "# print(validation_df.head())\n",
    "\n",
    "# print(\"Test data preview:\")\n",
    "# print(test_df.head())\n",
    "\n",
    "# # Adjust the preprocessing function based on actual column names\n",
    "# def preprocess_data(df, tokenizer, max_length=128):\n",
    "#     sentence1_col = 'sentence1'\n",
    "#     sentence2_col = 'sentence2'\n",
    "#     label_col = 'gold_label'\n",
    "    \n",
    "#     inputs = tokenizer(\n",
    "#         df[sentence1_col].tolist(),\n",
    "#         df[sentence2_col].tolist(),\n",
    "#         padding=True,\n",
    "#         truncation=True,\n",
    "#         max_length=max_length,\n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "    \n",
    "#     label_mapping = {'contradiction': 0, 'entailment': 1, 'neutral': 2}\n",
    "#     labels = df[label_col].map(label_mapping).tolist()\n",
    "    \n",
    "#     return inputs, torch.tensor(labels)\n",
    "\n",
    "# # Preprocess the data\n",
    "# train_inputs, train_labels = preprocess_data(train_df, tokenizer)\n",
    "# val_inputs, val_labels = preprocess_data(validation_df, tokenizer)\n",
    "# test_inputs, test_labels = preprocess_data(test_df, tokenizer)\n",
    "\n",
    "# # Display the processed data\n",
    "# print(\"Train inputs:\", train_inputs)\n",
    "# print(\"Train labels:\", train_labels)\n",
    "# print(\"Validation inputs:\", val_inputs)\n",
    "# print(\"Validation labels:\", val_labels)\n",
    "# print(\"Test inputs:\", test_inputs)\n",
    "# print(\"Test labels:\", test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns: Index(['annotator_labels', 'genre', 'gold_label', 'pairID', 'promptID',\n",
      "       'sentence1', 'translation1', 'sentence2', 'translation2'],\n",
      "      dtype='object')\n",
      "Validation columns: Index(['annotator_labels', 'genre', 'gold_label', 'pairID', 'promptID',\n",
      "       'sentence1', 'translation1', 'sentence2', 'translation2'],\n",
      "      dtype='object')\n",
      "Test columns: Index(['annotator_labels', 'genre', 'gold_label', 'pairID', 'promptID',\n",
      "       'sentence1', 'translation1', 'sentence2', 'translation2'],\n",
      "      dtype='object')\n",
      "Train data preview:\n",
      "    annotator_labels    genre     gold_label   pairID  promptID  \\\n",
      "0     ['entailment']  fiction     entailment  134793e    134793   \n",
      "1     ['entailment']  fiction     entailment   37397e     37397   \n",
      "2     ['entailment']    slate     entailment    1069e      1069   \n",
      "3  ['contradiction']    slate  contradiction   23505c     23505   \n",
      "4  ['contradiction']   travel  contradiction   60529c     60529   \n",
      "\n",
      "                                           sentence1  \\\n",
      "0  One of our number will carry out your instruct...   \n",
      "1  How do you know? All this is their information...   \n",
      "2   (Read  for Slate 's take on Jackson's findings.)   \n",
      "3                                 Gays and lesbians.   \n",
      "4  At the end of Rue des Francs-Bourgeois is what...   \n",
      "\n",
      "                                        translation1  \\\n",
      "0           אחד מאיתנו יבצע את ההוראות שלך בדקדקנות.   \n",
      "1              מאיפה אתה יודע? כל זה שוב המידע שלהם.   \n",
      "2      (קראו את הדעה של Slate על הממצאים של ג'קסון.)   \n",
      "3                                   הומואים ולסביות.   \n",
      "4  בסוף רחוב דה פרנק-בורגואה נמצא מה שרבים מחשיבי...   \n",
      "\n",
      "                                           sentence2  \\\n",
      "0  A member of my team will execute your orders w...   \n",
      "1                  This information belongs to them.   \n",
      "2        Slate had an opinion on Jackson's findings.   \n",
      "3                                     Heterosexuals.   \n",
      "4  Place des Vosges is constructed entirely of gr...   \n",
      "\n",
      "                                  translation2  \n",
      "0  חבר בצוות שלי יבצע את הפקודות שלך בדיוק רב.  \n",
      "1                          המידע הזה שייך להם.  \n",
      "2       ל-Slate הייתה דעה על ממצאיו של ג'קסון.  \n",
      "3                                הטרוסקסואלים.  \n",
      "4            כיכר דה ווז בנויה כולה משיש אפור.  \n",
      "Validation data preview:\n",
      "    annotator_labels       genre     gold_label   pairID  promptID  \\\n",
      "0        ['neutral']      travel        neutral   52772n     52772   \n",
      "1        ['neutral']  government        neutral  141334n    141334   \n",
      "2  ['contradiction']       slate  contradiction   13806c     13806   \n",
      "3     ['entailment']       slate     entailment  131294e    131294   \n",
      "4     ['entailment']     fiction     entailment   19833e     19833   \n",
      "\n",
      "                                           sentence1  \\\n",
      "0  Thebes held onto power until the 12th Dynasty,...   \n",
      "1  Without the support of federal finance organiz...   \n",
      "2  The win brought tears to her eyes, of course, ...   \n",
      "3  The New York Times ' Janet Maslin says the fil...   \n",
      "4              The next witness was Mary Cavendish.    \n",
      "\n",
      "                                        translation1  \\\n",
      "0  תבאי החזיקה בכוח עד השושלת ה-12, כאשר מלכה הרא...   \n",
      "1  ללא תמיכת ארגוני המימון הפדרליים, מנהלי התוכני...   \n",
      "2  הניצחון הביא דמעות לעיניה, כמובן, ועורר שיחת ט...   \n",
      "3  ג'נט מסלין מהניו יורק טיימס אומרת שהסרט עובד ב...   \n",
      "4                       העדה הבאה הייתה מרי קוונדיש.   \n",
      "\n",
      "                                           sentence2  \\\n",
      "0  The capital near Memphis lasted only half a ce...   \n",
      "1  Federal finance organizations are the best sou...   \n",
      "2  She called her father after she lost, smiling ...   \n",
      "3       The movie stars Matt Dillon and Gary Sinise.   \n",
      "4                         There was another witness.   \n",
      "\n",
      "                                        translation2  \n",
      "0  הבירה ליד ממפיס החזיקה מעמד רק חצי מאה לפני שת...  \n",
      "1  ארגוני מימון פדרליים הם המקור הטוב ביותר למידע...  \n",
      "2  היא התקשרה לאביה לאחר שהפסידה, מחייכת בזמן שעש...  \n",
      "3               כוכבי הקולנוע מאט דילון וגארי סיניז.  \n",
      "4                                       היה עד נוסף.  \n",
      "Test data preview:\n",
      "    annotator_labels    genre     gold_label   pairID  promptID  \\\n",
      "0        ['neutral']  fiction        neutral  128144n    128144   \n",
      "1        ['neutral']    slate        neutral  145721n    145721   \n",
      "2     ['entailment']   travel     entailment   79085e     79085   \n",
      "3  ['contradiction']  fiction  contradiction  133206c    133206   \n",
      "4  ['contradiction']    slate  contradiction  107785c    107785   \n",
      "\n",
      "                                           sentence1  \\\n",
      "0                                   She smiled back.   \n",
      "1  But such a show would have meant the museum ta...   \n",
      "2  In June you can watch dealers haggling over he...   \n",
      "3  119 Cynthia was back from the hospital, and I ...   \n",
      "4  Oh well, if we're not destined for matrimony i...   \n",
      "\n",
      "                                        translation1  \\\n",
      "0                                  היא חייכה בחזרה.    \n",
      "1  אבל מופע כזה היה אומר שהמוזיאון יסתכל טוב על ה...   \n",
      "2  בחודש יוני אפשר לצפות בסוחרים מתמקחים על ערמות...   \n",
      "3  סינתיה חזרה מבית החולים, ואני הצבתי את הכיסא ש...   \n",
      "4  ובכן, אם לא נועדנו לנישואין בחיים האלה, אולי ה...   \n",
      "\n",
      "                                           sentence2  \\\n",
      "0        She was so happy she couldn't stop smiling.   \n",
      "1  The museum didn't want to look too closely int...   \n",
      "2  During the summer months dealers barter for pi...   \n",
      "3  Poirot talked to Cynthia himself, asking to vi...   \n",
      "4  If we are married, I will buy you a beer or a ...   \n",
      "\n",
      "                                        translation2  \n",
      "0     היא היתה כל כך שמחה שהיא לא יכלה להפסיק לחייך.  \n",
      "1  המוזיאון לא רצה להתבונן מקרוב מדי בחלקו בעולם ...  \n",
      "2  בחודשי הקיץ סוחרים סוחרים בערימות של פקעות תול...  \n",
      "3  פוארו דיבר עם סינתיה בעצמו, וביקש לבקר אותה בע...  \n",
      "4                 אם נתחתן, אקנה לך בירה או ד\"ר פפר.  \n"
     ]
    }
   ],
   "source": [
    "#this one\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Disable parallelism warning for tokenizers\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"bert-base-multilingual-cased\"  # Change this to AlephBERT or DictaBERT as needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "# Define the paths to the local files\n",
    "# train_file = \"/Users/user/Library/Mobile Documents/com~apple~CloudDocs/2 24/NLP/final proj/data/train.csv\"\n",
    "# validation_file = \"/Users/user/Library/Mobile Documents/com~apple~CloudDocs/2 24/NLP/final proj/data/dev.csv\"\n",
    "# test_file = \"/Users/user/Library/Mobile Documents/com~apple~CloudDocs/2 24/NLP/final proj/data/test.csv\"\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv(train_file)\n",
    "validation_df = pd.read_csv(validation_file)\n",
    "test_df = pd.read_csv(test_file)\n",
    "\n",
    "# Display the columns in each DataFrame\n",
    "print(\"Train columns:\", train_df.columns)\n",
    "print(\"Validation columns:\", validation_df.columns)\n",
    "print(\"Test columns:\", test_df.columns)\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "print(\"Train data preview:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"Validation data preview:\")\n",
    "print(validation_df.head())\n",
    "\n",
    "print(\"Test data preview:\")\n",
    "print(test_df.head())\n",
    "\n",
    "# Adjust the preprocessing function based on actual column names\n",
    "def preprocess_data(df, tokenizer, max_length=128):\n",
    "    sentence1_col = 'sentence1'\n",
    "    sentence2_col = 'sentence2'\n",
    "    label_col = 'gold_label'\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        df[sentence1_col].tolist(),\n",
    "        df[sentence2_col].tolist(),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    label_mapping = {'contradiction': 0, 'entailment': 1, 'neutral': 2}\n",
    "    labels = df[label_col].map(label_mapping).tolist()\n",
    "    \n",
    "    return inputs, torch.tensor(labels)\n",
    "\n",
    "# Preprocess the data\n",
    "train_inputs, train_labels = preprocess_data(train_df, tokenizer)\n",
    "val_inputs, val_labels = preprocess_data(validation_df, tokenizer)\n",
    "test_inputs, test_labels = preprocess_data(test_df, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Data loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "def create_data_loader(inputs, labels, batch_size=16):\n",
    "    dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], inputs['token_type_ids'], labels)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return data_loader\n",
    "\n",
    "train_loader = create_data_loader(train_inputs, train_labels)\n",
    "val_loader = create_data_loader(val_inputs, val_labels)\n",
    "test_loader = create_data_loader(test_inputs, test_labels)\n",
    "\n",
    "\n",
    "#subset part:\n",
    "# # Create DataLoader\n",
    "# def create_data_loader(inputs, labels, batch_size=16):\n",
    "#     dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], inputs['token_type_ids'], labels)\n",
    "#     data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "#     return data_loader\n",
    "\n",
    "# # Use a subset of the training data\n",
    "# subset_size = 100  # Use 100 samples for quick testing\n",
    "# subset_train_dataset = TensorDataset(train_inputs['input_ids'][:subset_size], \n",
    "#                                      train_inputs['attention_mask'][:subset_size], \n",
    "#                                      train_inputs['token_type_ids'][:subset_size], \n",
    "#                                      train_labels[:subset_size])\n",
    "# subset_train_loader = DataLoader(subset_train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# # Validation and Test DataLoader\n",
    "# val_loader = create_data_loader(val_inputs, val_labels)\n",
    "# test_loader = create_data_loader(test_inputs, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hadare/Documents/CodingProjects/SBERT/sbert/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Check for MPS and CUDA availability\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"MPS is available\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"CUDA is available\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Training setup\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop on a subset of the data\n",
    "# epochs = 2  # Run only for 1 epoch for quick testing\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for batch in tqdm(subset_train_loader):\n",
    "#         input_ids, attention_mask, token_type_ids, labels = [b.to(device) for b in batch]\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "#         logits = outputs.logits  # Assuming model returns logits as the first output\n",
    "#         loss = loss_fn(logits, labels)\n",
    "#         total_loss += loss.item()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     avg_train_loss = total_loss / len(subset_train_loader)\n",
    "#     print(f'Epoch {epoch + 1}, Loss: {avg_train_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18332/18332 [2:41:52<00:00,  1.89it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18332/18332 [2:06:13<00:00,  2.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.4573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18332/18332 [2:18:01<00:00,  2.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.3649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "    \n",
    "        input_ids, attention_mask, token_type_ids, labels = [b.to(device) for b in batch]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        logits = outputs.logits  # Assuming model returns logits as the first output\n",
    "        loss = loss_fn(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_train_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./trained_sbert_model/tokenizer_config.json',\n",
       " './trained_sbert_model/special_tokens_map.json',\n",
       " './trained_sbert_model/vocab.txt',\n",
       " './trained_sbert_model/added_tokens.json',\n",
       " './trained_sbert_model/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save_pretrained('./trained_sbert_model')\n",
    "tokenizer.save_pretrained('./trained_sbert_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8042\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test set\n",
    "model.eval()\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, token_type_ids, labels = [b.to(device) for b in batch]\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algo no 1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
